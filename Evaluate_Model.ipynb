{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bff10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, sys, argparse\n",
    "import tensorflow.keras.backend as K\n",
    "#from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, TerminateOnNaN\n",
    "\n",
    "from hourglass.model import get_hourglass_model\n",
    "from hourglass.data import hourglass_dataset\n",
    "from hourglass.loss import get_loss\n",
    "from hourglass.callbacks import EvalCallBack, CheckpointCleanCallBack\n",
    "from common.utils import get_classes, get_matchpoints, get_model_type, optimize_tf_gpu\n",
    "from common.model_utils import get_optimizer\n",
    "from hourglass.postprocess import post_process_heatmap_simple\n",
    "from tensorflow.keras.models import load_model\n",
    "from eval import keypoint_accuracy\n",
    "from common.model_utils import get_normalize\n",
    "from tqdm import tqdm\n",
    "# Try to enable Auto Mixed Precision on TF 2.0\n",
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "# os.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'] = '1'\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "# optimize_tf_gpu(tf, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc045847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments parsed from command line can be set here\n",
    "# Model definition options:\n",
    "num_stacks=2\n",
    "mobile=False\n",
    "tiny=False\n",
    "model_input_shape=\"256x256\"\n",
    "weights_path=None\n",
    "\n",
    "# Data options\n",
    "dataset_path=\"data/mpii\"\n",
    "classes_path=\"configs/mpii_classes.txt\"\n",
    "matchpoint_path=\"configs/mpii_match_point.txt\"\n",
    "\n",
    "# Training options\n",
    "batch_size=8\n",
    "optimizer=\"RMSProp\"\n",
    "loss_type=\"mse\"\n",
    "learning_rate=5e-4\n",
    "decay_type=None\n",
    "mixed_precision=False\n",
    "init_epoch=0\n",
    "total_epoch=100\n",
    "gpu_num=1\n",
    "\n",
    "height, width = model_input_shape.split('x')\n",
    "model_input_shape = (int(height), int(width)) \n",
    "orig_img_shape = (1280, 720)    # Height and width in pixels of input images, can be read from file or manually specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8e52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "log_dir = 'logs/000'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "if matchpoint_path:\n",
    "    matchpoints = get_matchpoints(matchpoint_path)\n",
    "else:\n",
    "    matchpoints = None\n",
    "\n",
    "# choose model type\n",
    "if tiny:\n",
    "    num_channels = 128\n",
    "else:\n",
    "    num_channels = 256\n",
    "\n",
    "if mixed_precision:\n",
    "    tf_major_version = float(tf.__version__[:3])\n",
    "    if tf_major_version >= 2.1:\n",
    "        # apply mixed_precision for valid TF version\n",
    "        from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_policy(policy)\n",
    "    else:\n",
    "        raise ValueError('Tensorflow {} does not support mixed precision'.format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3bbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = (int(model_input_shape[0]/4), int(model_input_shape[1]/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5933e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.data_utils import random_horizontal_flip, random_vertical_flip, random_brightness\n",
    "from common.data_utils import random_grayscale, random_chroma, random_contrast, random_sharpness, random_blur, random_histeq, random_rotate_angle\n",
    "from common.data_utils import crop_single_object, rotate_single_object, crop_image, normalize_image, transform_keypoints, generate_gt_heatmap\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def map_dataset_to_image_heatmaps_val(imagefile, center, scale, keypoints):\n",
    "            \n",
    "    img = tf.io.read_file(imagefile)\n",
    "    decoded_img = tf.io.decode_png(img, channels=3)\n",
    "#     orig_img_shape = decoded_img.shape\n",
    "    resized_img = tf.image.resize(decoded_img, model_input_shape)\n",
    "    image = resized_img\n",
    "    \n",
    "#     image = tf.expand_dims(resized_img, axis=0)\n",
    "\n",
    "\n",
    "#     img = tf.io.read_file(imagefile)\n",
    "#     if img.mode != 'RGB':\n",
    "#         img = img.convert('RGB')\n",
    "#     image = np.array(img)\n",
    "#     img.close()\n",
    "    \n",
    "#     image_shape = image.shape\n",
    "\n",
    "    rotate_angle = 0\n",
    "#     image = crop_image(image, center, scale, model_input_shape, rotate_angle)\n",
    "    \n",
    "    # transform keypoints to cropped image reference\n",
    "#     transformed_keypoints = transform_keypoints(keypoints, center, scale, output_shape, rotate_angle)\n",
    "\n",
    "        # in case we got an empty image, bypass the sample\n",
    "#     if image is None:\n",
    "#         return None, None, None\n",
    "    \n",
    "    # normalize image\n",
    "#     image = normalize_image(image, self.get_color_mean())\n",
    "\n",
    "\n",
    "\n",
    "    # Data Augmentation\n",
    "#     image, keypoints = crop_single_object(image, keypoints, center, scale, model_input_shape)\n",
    "#     seed = tf.random.uniform(shape=[2], maxval=3, dtype=tf.int32)\n",
    "#     image = tf.image.stateless_random_brightness(image, max_delta=0.95, seed=seed)\n",
    "#     image = tf.image.stateless_random_contrast(image, lower=0.1, upper=0.9, seed=seed)\n",
    "#     image = tf.image.stateless_random_hue(image, 0.2, seed)\n",
    "#     image = tf.image.stateless_random_jpeg_quality(image, 75, 95, seed)\n",
    "#     image = tf.image.stateless_random_saturation(image, lower=0.5, upper=1.0, seed=seed)\n",
    "\n",
    "\n",
    "    # Rescale keypoints from original_img_shape to output_shape\n",
    "    resized_scale = tf.divide(orig_img_shape, output_shape)\n",
    "    resized_scale = tf.concat([resized_scale, [1]], 0)  # adding third dimension for visibility dimension in the keypoints \n",
    "    keypoints = tf.multiply(keypoints, resized_scale)\n",
    "\n",
    "    # generate ground truth keypoint heatmap\n",
    "#     gt_heatmap = generate_gt_heatmap(keypoints, output_shape)\n",
    "\n",
    "#     out_heatmaps = []\n",
    "#     for m in range(num_stacks):\n",
    "#         out_heatmaps.append(gt_heatmap)\n",
    "            \n",
    "#     return (image, tf.stack(out_heatmaps, axis=-1))\n",
    "\n",
    "    return (image, keypoints)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5e32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def dataset_from_annotations(annotations, image_path, validation_set=False):\n",
    "\n",
    "    image_filenames = []\n",
    "    centers = []\n",
    "    keypoints = []\n",
    "    scales = []\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        image_filename = os.path.join(image_path, annotation['img_paths'])\n",
    "        center = np.array(annotation['objpos'])\n",
    "        keypoint = np.array(annotation['joint_self'])\n",
    "        scale = annotation['scale_provided']\n",
    "        \n",
    "        # adjust center/scale slightly to avoid cropping limbs\n",
    "\n",
    "        if center[0] != -1:\n",
    "            center[1] = center[1] + 15 * scale\n",
    "            scale = scale * 1.25\n",
    "        \n",
    "        if annotation['isValidation'] == validation_set:        \n",
    "            image_filenames.append(image_filename)\n",
    "            centers.append(center)\n",
    "            keypoints.append(keypoint)\n",
    "            scales.append(scale)\n",
    "        else:\n",
    "            pass\n",
    "    img_filenames = tf.convert_to_tensor(image_filenames)\n",
    "    img_centers = tf.convert_to_tensor(centers)\n",
    "    img_scales = tf.convert_to_tensor(scales)\n",
    "    img_keypoints = tf.convert_to_tensor(keypoints)\n",
    "    return image_filenames, centers, scales, keypoints\n",
    "  \n",
    "                 \n",
    "            \n",
    "    \n",
    "        \n",
    "# dataset with tf.data.Dataset and training using model.fit()\n",
    "json_file = \"data/mpii/annotations.json\"\n",
    "image_path = \"data/mpii/images/\"\n",
    "with open(json_file) as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0f5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_filenames, img_centers, img_scales, img_keypoints = dataset_from_annotations(annotations, image_path,validation_set=True)      \n",
    "tfdataset_val= tf.data.Dataset.from_tensor_slices((img_filenames, img_centers, img_scales, img_keypoints))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45623c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "tfdataset_mapped_val = tfdataset_val.map(map_dataset_to_image_heatmaps_val, num_parallel_calls=AUTOTUNE).batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e184aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"logs/000/ep024-loss0.001-val_acc0.725.h5\"\n",
    "model = load_model(model_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020d42e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 16, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdataset_mapped_val\n",
    "data_item = next(iter(tfdataset_mapped_val))\n",
    "data_item[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f92dfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfdataset_mapped_val.element_spec[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "949d2da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 64, 64, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(data_item[0])\n",
    "prediction[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac7312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_callback_new(model, tfdataset_mapped_val, batch_size, class_names, model_input_shape, threshold):\n",
    "    succeed_dict = {class_name: 0 for class_name in class_names}\n",
    "    fail_dict = {class_name: 0 for class_name in class_names}\n",
    "    accuracy_dict = {class_name: 0. for class_name in class_names}\n",
    "\n",
    "    normalize = get_normalize(model_input_shape)\n",
    "#     threshold = 0.5\n",
    "\n",
    "    pbar = tqdm(total=len(tfdataset_mapped_val), desc='Eval model')\n",
    "    for batch in iter(tfdataset_mapped_val):\n",
    "        prediction = model.predict_on_batch(batch[0])\n",
    "        pred_keypoints= tf.TensorArray(tf.double, size=batch_size)\n",
    "        for i in range(0,batch_size):\n",
    "            pred_keypoints = pred_keypoints.write(i,post_process_heatmap_simple(prediction[1][i]))\n",
    "            result_list = keypoint_accuracy(pred_keypoints.stack()[i], batch[1][i], threshold, normalize)\n",
    "            for i, class_name in enumerate(class_names):\n",
    "                if result_list[i] == 0:\n",
    "                    fail_dict[class_name] = fail_dict[class_name] + 1\n",
    "                elif result_list[i] == 1:\n",
    "                    succeed_dict[class_name] = succeed_dict[class_name] + 1\n",
    "\n",
    "\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        accuracy_dict[class_name] = succeed_dict[class_name] * 1.0 / (succeed_dict[class_name] + fail_dict[class_name])\n",
    "\n",
    "    total_succeed = np.sum(list(succeed_dict.values()))\n",
    "    total_fail = np.sum(list(fail_dict.values()))\n",
    "    total_accuracy = total_succeed * 1.0 / (total_fail + total_succeed)\n",
    "    \n",
    "    return total_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce07871",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback_new(tfdataset_mapped_val, batch_size, class_names, model_input_shape, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "succeed_dict = {class_name: 0 for class_name in class_names}\n",
    "fail_dict = {class_name: 0 for class_name in class_names}\n",
    "accuracy_dict = {class_name: 0. for class_name in class_names}\n",
    "\n",
    "normalize = get_normalize(model_input_shape)\n",
    "threshold = 0.5\n",
    "\n",
    "pbar = tqdm(total=47, desc='Eval model')\n",
    "for batch in iter(tfdataset_mapped_val):\n",
    "    prediction = model.predict_on_batch(batch[0])\n",
    "    pred_keypoints= tf.TensorArray(tf.double, size=batch_size)\n",
    "    for i in range(0,batch_size):\n",
    "        pred_keypoints = pred_keypoints.write(i,post_process_heatmap_simple(prediction[1][i]))\n",
    "        result_list = keypoint_accuracy(pred_keypoints.stack()[i], batch[1][i], threshold, normalize)\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            if result_list[i] == 0:\n",
    "                fail_dict[class_name] = fail_dict[class_name] + 1\n",
    "            elif result_list[i] == 1:\n",
    "                succeed_dict[class_name] = succeed_dict[class_name] + 1\n",
    "\n",
    "        \n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    accuracy_dict[class_name] = succeed_dict[class_name] * 1.0 / (succeed_dict[class_name] + fail_dict[class_name])\n",
    "\n",
    "total_succeed = np.sum(list(succeed_dict.values()))\n",
    "total_fail = np.sum(list(fail_dict.values()))\n",
    "total_accuracy = total_succeed * 1.0 / (total_fail + total_succeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb7b2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "pred_keypoints = tf.TensorArray(tf.double, size=batch_size)\n",
    "pred_keypoints = pred_keypoints.write(i, post_process_heatmap_simple(prediction[1][i], 0.001))\n",
    "pred_keypoints.stack()[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab09698a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(tfdataset_mapped_val))\n",
    "prediction = model.predict_on_batch(batch[0])\n",
    "prediction[1][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dba31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_keypoints = post_process_heatmap_simple(prediction[1][:,:,1,:], 0.1)\n",
    "tf.stack(pred_keypoints).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e08696",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item[1][60].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed496a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = keypoint_accuracy(pred_keypoints.stack()[0], data_item[1][0], 0.5, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "succeed_dict = {class_name: 0 for class_name in class_names}\n",
    "fail_dict = {class_name: 0 for class_name in class_names}\n",
    "accuracy_dict = {class_name: 0. for class_name in class_names}\n",
    "\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    if result_list[i] == 0:\n",
    "        fail_dict[class_name] = fail_dict[class_name] + 1\n",
    "    elif result_list[i] == 1:\n",
    "        succeed_dict[class_name] = succeed_dict[class_name] + 1\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    accuracy_dict[class_name] = succeed_dict[class_name] * 1.0 / (succeed_dict[class_name] + fail_dict[class_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca00482",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_succeed = np.sum(list(succeed_dict.values()))\n",
    "total_fail = np.sum(list(fail_dict.values()))\n",
    "total_accuracy = total_succeed * 1.0 / (total_fail + total_succeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ba494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0aea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
