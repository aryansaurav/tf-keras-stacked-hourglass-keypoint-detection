{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bff10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, sys, argparse\n",
    "import tensorflow.keras.backend as K\n",
    "#from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, TerminateOnNaN\n",
    "\n",
    "from hourglass.model import get_hourglass_model\n",
    "from hourglass.data import hourglass_dataset\n",
    "from hourglass.loss import get_loss\n",
    "from hourglass.callbacks import EvalCallBack, CheckpointCleanCallBack, EvalCallBackNew\n",
    "from common.utils import get_classes, get_matchpoints, get_model_type, optimize_tf_gpu\n",
    "from common.model_utils import get_optimizer\n",
    "from common.data_utils import generate_gt_heatmap\n",
    "# Try to enable Auto Mixed Precision on TF 2.0\n",
    "# os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "# os.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'] = '1'\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# optimize_tf_gpu(tf, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc045847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments parsed from command line can be set here\n",
    "# Model definition options:\n",
    "num_stacks=2\n",
    "mobile=True\n",
    "tiny=True\n",
    "model_input_shape=\"256x256\"\n",
    "weights_path=None\n",
    "\n",
    "# Data options\n",
    "dataset_path=\"data/mpii\"\n",
    "classes_path=\"configs/mpii_classes.txt\"\n",
    "matchpoint_path=\"configs/mpii_match_point.txt\"\n",
    "\n",
    "# Training options\n",
    "batch_size=16\n",
    "optimizer=\"RMSProp\"\n",
    "loss_type=\"mse\"\n",
    "learning_rate=5e-4\n",
    "decay_type=None\n",
    "mixed_precision=False\n",
    "init_epoch=0\n",
    "total_epoch=100\n",
    "gpu_num=1\n",
    "\n",
    "height, width = model_input_shape.split('x')\n",
    "model_input_shape = (int(height), int(width)) \n",
    "orig_img_shape = (1280, 720)    # Height and width in pixels of input images, can be read from file or manually specified\n",
    "output_shape = (int(model_input_shape[0]/4), int(model_input_shape[1]/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8e52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "log_dir = 'logs/000'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "if matchpoint_path:\n",
    "    matchpoints = get_matchpoints(matchpoint_path)\n",
    "else:\n",
    "    matchpoints = None\n",
    "\n",
    "# choose model type\n",
    "if tiny:\n",
    "    num_channels = 128\n",
    "else:\n",
    "    num_channels = 256\n",
    "\n",
    "if mixed_precision:\n",
    "    tf_major_version = float(tf.__version__[:3])\n",
    "    if tf_major_version >= 2.1:\n",
    "        # apply mixed_precision for valid TF version\n",
    "        from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_policy(policy)\n",
    "    else:\n",
    "        raise ValueError('Tensorflow {} does not support mixed precision'.format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd50474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get train/val dataset\n",
    "# train_generator = hourglass_dataset(dataset_path, batch_size, class_names,\n",
    "#                                     input_shape=model_input_shape,\n",
    "#                                     num_hgstack=num_stacks,\n",
    "#                                     is_train=True,\n",
    "#                                     with_meta=False,\n",
    "#                                     matchpoints=matchpoints)\n",
    "\n",
    "# num_train = train_generator.get_dataset_size()\n",
    "# num_val = len(train_generator.get_val_annotations())\n",
    "\n",
    "# model_type = get_model_type(num_stacks, mobile, tiny, model_input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "488cf40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "resized_scale = np.divide(output_shape, orig_img_shape)\n",
    "resized_scale = np.append(resized_scale, [1.0])\n",
    "\n",
    "def dataset_from_annotations(annotations, image_path, validation_set=False):\n",
    "\n",
    "    image_filenames = []\n",
    "    centers = []\n",
    "    keypoints = []\n",
    "    scales = []\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        image_filename = os.path.join(image_path, annotation['img_paths'])\n",
    "        center = np.array(annotation['objpos'])\n",
    "        keypoint = np.array(annotation['joint_self'])\n",
    "        scale = annotation['scale_provided']\n",
    "        \n",
    "        # adjust center/scale slightly to avoid cropping limbs\n",
    "\n",
    "        if center[0] != -1:\n",
    "            center[1] = center[1] + 15 * scale\n",
    "            scale = scale * 1.25\n",
    "        \n",
    "        if annotation['isValidation'] == validation_set:        \n",
    "            image_filenames.append(image_filename)\n",
    "            centers.append(center)\n",
    "            keypoints.append(keypoint*resized_scale)\n",
    "            scales.append(scale)\n",
    "        else:\n",
    "            pass\n",
    "    img_filenames = tf.convert_to_tensor(image_filenames)\n",
    "    img_centers = tf.convert_to_tensor(centers, dtype=tf.float32)\n",
    "    img_scales = tf.convert_to_tensor(scales, dtype=tf.float32)\n",
    "    img_keypoints = tf.convert_to_tensor(keypoints, dtype=tf.float32)\n",
    "#     return image_filenames, centers, scales, keypoints\n",
    "    return image_filenames, keypoints, centers, scales\n",
    "  \n",
    "                 \n",
    "            \n",
    "    \n",
    "        \n",
    "# dataset with tf.data.Dataset and training using model.fit()\n",
    "json_file = \"data/mpii/annotations.json\"\n",
    "image_path = \"data/mpii/images/\"\n",
    "with open(json_file) as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f797d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "img_filenames_train, img_keypoints_train, img_centers_train, img_scales_train = dataset_from_annotations(annotations, image_path,validation_set=False)   \n",
    "tfdataset_train= tf.data.Dataset.from_tensor_slices((img_filenames_train, img_keypoints_train))\n",
    "\n",
    "img_filenames_test, img_keypoints_test, img_centers_test, img_scales_test = dataset_from_annotations(annotations, image_path,validation_set=True)      \n",
    "tfdataset_val= tf.data.Dataset.from_tensor_slices((img_filenames_test, img_keypoints_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a4c67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TRAIN= True\n",
    "\n",
    "def map_image_open(image_filename):\n",
    "    image = tf.io.decode_png(tf.io.read_file(image_filename))\n",
    "    img_size = tf.shape(image)[0:2]\n",
    "       \n",
    "\n",
    "\n",
    "    return (tf.image.resize(image, model_input_shape), img_size)\n",
    "\n",
    "def map_rescale_keypoints(image_data, keypoint_data): \n",
    "    # Simply rescale the keypoints by the factor of model_output_shape to original image shape\n",
    "    scale_factor = tf.divide(output_shape, image_data[1])\n",
    "    scale_factor = tf.concat([scale_factor, [1]], 0)\n",
    "    rescaled_keypoints = tf.cast(tf.multiply(keypoint_data, scale_factor), dtype=tf.float32)\n",
    "    \n",
    "    gt_heatmap = generate_gt_heatmap(rescaled_keypoints, output_shape)\n",
    "\n",
    "    out_heatmaps = []\n",
    "    for m in range(num_stacks):\n",
    "        out_heatmaps.append(gt_heatmap)\n",
    "    if IS_TRAIN:\n",
    "        # Data Augmentation\n",
    "        #image, keypoints = crop_single_object(image, keypoints, center, scale, model_input_shape)\n",
    "        seed = tf.random.uniform(shape=[2], maxval=3, dtype=tf.int32)\n",
    "        image = tf.image.stateless_random_brightness(image_data[0], max_delta=0.95, seed=seed)\n",
    "        image = tf.image.stateless_random_contrast(image_data[0], lower=0.1, upper=0.9, seed=seed)\n",
    "        image = tf.image.stateless_random_hue(image_data[0], 0.2, seed)\n",
    "        image = tf.image.stateless_random_jpeg_quality(image_data[0], 75, 95, seed)\n",
    "        image = tf.image.stateless_random_saturation(image_data[0], lower=0.5, upper=1.0, seed=seed)\n",
    "\n",
    "        return (image_data[0], tf.stack(out_heatmaps, axis=-1))   # Return the image tensor and rescaled keypoints\n",
    "    else:\n",
    "        return (image_data[0], rescaled_keypoints)\n",
    "\n",
    "# train_image_dataset = (tf.data.Dataset.from_tensor_slices(img_filenames_train)\n",
    "#            .map(lambda x: tf.image.resize(tf.io.decode_png(tf.io.read_file(x)), model_input_shape),\n",
    "#                 num_parallel_calls=AUTOTUNE)\n",
    "#            .prefetch(AUTOTUNE))\n",
    "\n",
    "# train_image_dataset = (tf.data.Dataset.from_tensor_slices(img_filenames_train)\n",
    "#            .map(lambda x: tf.io.decode_png(tf.io.read_file(x)),\n",
    "#                 num_parallel_calls=AUTOTUNE)\n",
    "#            .prefetch(AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2fc97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TRAIN = True\n",
    "train_image_dataset = (tf.data.Dataset.from_tensor_slices(img_filenames_train)\n",
    "           .map(map_image_open,\n",
    "                num_parallel_calls=AUTOTUNE)\n",
    "           .prefetch(AUTOTUNE))\n",
    "train_keypoints_dataset = (tf.data.Dataset.from_tensor_slices(img_keypoints_train).prefetch(AUTOTUNE))\n",
    "tfdataset_mapped_train = tf.data.Dataset.zip((train_image_dataset, train_keypoints_dataset)).map(map_rescale_keypoints).batch(batch_size).prefetch(AUTOTUNE)\n",
    "IS_TRAIN = False\n",
    "test_image_dataset = (tf.data.Dataset.from_tensor_slices(img_filenames_test)\n",
    "           .map(map_image_open,\n",
    "                num_parallel_calls=AUTOTUNE)\n",
    "           .prefetch(AUTOTUNE))\n",
    "test_keypoints_dataset = (tf.data.Dataset.from_tensor_slices(img_keypoints_test).prefetch(AUTOTUNE))\n",
    "tfdataset_mapped_val = tf.data.Dataset.zip((test_image_dataset, test_keypoints_dataset)).map(map_rescale_keypoints).batch(batch_size).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a2229b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = iter(train_dataset_zip)\n",
    "# batch = next(iterator)\n",
    "# for i in range(5):\n",
    "#     batch = next(iterator)\n",
    "#     cropped_img = crop_image(batch[0], img_centers_train[i], img_scales_train[i], model_input_shape, 0)\n",
    "\n",
    "#     print(\"Image shapes: \", batch[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a0dec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Previous version slower mapping functions\n",
    "# from PIL import Image\n",
    "   \n",
    "# # Rescale keypoints from original_img_shape to output_shape\n",
    "# # resized_scale = tf.divide(output_shape, orig_img_shape)\n",
    "# # resized_scale = tf.concat([resized_scale, [1]], 0)  # adding third dimension for visibility dimension in the keypoints \n",
    "\n",
    "\n",
    "# def map_dataset_to_image_heatmaps(imagefile,  keypoints):\n",
    "            \n",
    "#     img = tf.io.read_file(imagefile)\n",
    "#     decoded_img = tf.io.decode_png(img, channels=3, dtype=tf.dtypes.uint8)\n",
    "# #     orig_img_shape = decoded_img.shape\n",
    "#     resized_img = tf.image.resize(decoded_img, model_input_shape)\n",
    "#     image = resized_img\n",
    "    \n",
    "#     # Data Augmentation\n",
    "# #     image, keypoints = crop_single_object(image, keypoints, center, scale, model_input_shape)\n",
    "#     seed = tf.random.uniform(shape=[2], maxval=3, dtype=tf.int32)\n",
    "#     image = tf.image.stateless_random_brightness(image, max_delta=0.95, seed=seed)\n",
    "#     image = tf.image.stateless_random_contrast(image, lower=0.1, upper=0.9, seed=seed)\n",
    "#     image = tf.image.stateless_random_hue(image, 0.2, seed)\n",
    "#     image = tf.image.stateless_random_jpeg_quality(image, 75, 95, seed)\n",
    "#     image = tf.image.stateless_random_saturation(image, lower=0.5, upper=1.0, seed=seed)\n",
    "\n",
    "\n",
    "#     # Rescale keypoints from original_img_shape to output_shape -- Done in previous step\n",
    "# #     resized_scale = tf.divide(orig_img_shape, model_input_shape)\n",
    "# #     resized_scale = tf.concat([resized_scale, [1]], 0)  # adding third dimension for visibility dimension in the keypoints \n",
    "# #     keypoints = tf.multiply(keypoints, resized_scale)\n",
    "\n",
    "#     # generate ground truth keypoint heatmap\n",
    "#     gt_heatmap = generate_gt_heatmap(keypoints, output_shape)\n",
    "\n",
    "#     out_heatmaps = []\n",
    "#     for m in range(num_stacks):\n",
    "#         out_heatmaps.append(gt_heatmap)\n",
    "        \n",
    "#     return (image, tf.stack(out_heatmaps, axis=-1))\n",
    "\n",
    "\n",
    "# def map_dataset_to_image_heatmaps_val(imagefile,  keypoints):\n",
    "            \n",
    "#     img = tf.io.read_file(imagefile)\n",
    "#     decoded_img = tf.io.decode_png(img, channels=3)\n",
    "#     resized_img = tf.image.resize(decoded_img, model_input_shape)\n",
    "#     image = resized_img\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#     # Rescale keypoints from original_img_shape to output_shape   - Done in previous step already\n",
    "# #     resized_scale = tf.divide(orig_img_shape, output_shape)\n",
    "# #     resized_scale = tf.concat([resized_scale, [1]], 0)  # adding third dimension for visibility dimension in the keypoints \n",
    "# #     keypoints = tf.cast(tf.multiply(keypoints, resized_scale), dtype=tf.float32)\n",
    "# #     keypoints = tf.multiply(keypoints, resized_scale)\n",
    "\n",
    "\n",
    "#     return (image, keypoints)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "920f2fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "# tfdataset_mapped_train = tfdataset_train.map(map_dataset_to_image_heatmaps, num_parallel_calls=AUTOTUNE).batch(batch_size).prefetch(AUTOTUNE)\n",
    "# tfdataset_mapped_val = tfdataset_val.map(map_dataset_to_image_heatmaps_val, num_parallel_calls=AUTOTUNE).batch(batch_size, drop_remainder=True).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0645356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 16, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(tfdataset_mapped_val))\n",
    "item[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da8148b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = tfdataset_mapped_train.cardinality()*batch_size\n",
    "num_val = tfdataset_mapped_val.cardinality()*batch_size\n",
    "\n",
    "model_type = get_model_type(num_stacks, mobile, tiny, model_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3f89825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks for training process\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_grads=False, write_images=False, update_freq='batch')\n",
    "# eval_callback = EvalCallBack(log_dir, dataset_path, class_names, model_input_shape, model_type)\n",
    "eval_callback = EvalCallBackNew(log_dir, tfdataset_mapped_val, class_names, model_input_shape, model_type)\n",
    "checkpoint_clean = CheckpointCleanCallBack(log_dir, max_val_keep=5)\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [tensorboard, eval_callback, terminate_on_nan, checkpoint_clean]\n",
    "# callbacks = [tensorboard, terminate_on_nan, checkpoint_clean]\n",
    "\n",
    "# prepare optimizer\n",
    "steps_per_epoch = max(1, num_train//batch_size)\n",
    "decay_steps = steps_per_epoch * (total_epoch - init_epoch)\n",
    "optimizer = get_optimizer(optimizer, learning_rate, decay_type=decay_type, decay_steps=decay_steps)\n",
    "#optimizer = RMSprop(lr=5e-4)\n",
    "\n",
    "# prepare loss function\n",
    "loss_func = get_loss(loss_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "866ae498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Mobile Stacked Hourglass model with stack number 2, channel number 128. train input shape (256, 256)\n"
     ]
    }
   ],
   "source": [
    "# support multi-gpu training\n",
    "if gpu_num >= 2:\n",
    "    # devices_list=[\"/gpu:0\", \"/gpu:1\"]\n",
    "    devices_list=[\"/gpu:{}\".format(n) for n in range(gpu_num)]\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=devices_list)\n",
    "    print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    with strategy.scope():\n",
    "        # get multi-gpu train model. you can also use \"model_input_shape=None\" to create a dynamic input shape model,\n",
    "        # but multiscale train/inference doesn't work for it\n",
    "        model = get_hourglass_model(num_classes, num_stacks, num_channels, model_input_shape=model_input_shape, mobile=mobile)\n",
    "        # compile model\n",
    "        model.compile(optimizer=optimizer, loss=loss_func)\n",
    "else:\n",
    "    # get normal train model. you can also use \"model_input_shape=None\" to create a dynamic input shape model,\n",
    "    # but multiscale train/inference doesn't work for it\n",
    "    model = get_hourglass_model(num_classes, num_stacks, num_channels, model_input_shape=model_input_shape, mobile=mobile)\n",
    "    # compile model\n",
    "    model.compile(optimizer=optimizer, loss=loss_func)\n",
    "\n",
    "print('Create {} Stacked Hourglass model with stack number {}, channel number {}. train input shape {}'.format('Mobile' if mobile else '', num_stacks, num_channels, model_input_shape))\n",
    "# model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd29ae61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.7025642>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item = next(iter(tfdataset_mapped_val))\n",
    "tf.math.reduce_max(data_item[1][:,:,0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0c42635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing GPU device configuration\n",
    "# import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# tf.config.set_logical_device_configuration(\n",
    "#     physical_devices[0],\n",
    "#     [tf.config.LogicalDeviceConfiguration(memory_limit=100),\n",
    "#      tf.config.LogicalDeviceConfiguration(memory_limit=100)])\n",
    "# logical_devices = tf.config.list_logical_devices('GPU')\n",
    "# logical_devicesphysical_devices = tf.config.list_physical_devices('GPU')\n",
    "# try:\n",
    "#   tf.config.set_logical_device_configuration(\n",
    "#     physical_devices[0],\n",
    "#     [tf.config.LogicalDeviceConfiguration(memory_limit=100),\n",
    "#      tf.config.LogicalDeviceConfiguration(memory_limit=100)])\n",
    "\n",
    "#   logical_devices = tf.config.list_logical_devices('GPU')\n",
    "#   assert len(logical_devices) == len(physical_devices) + 1\n",
    "\n",
    "#   tf.config.set_logical_device_configuration(\n",
    "#     physical_devices[0],\n",
    "#     [tf.config.LogicalDeviceConfiguration(memory_limit=10),\n",
    "#      tf.config.LogicalDeviceConfiguration(memory_limit=10)])\n",
    "# except:\n",
    "#   # Invalid device or cannot modify logical devices once initialized.\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2589e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path=None\n",
    "# weights_path=\"logs/000/ep035-loss0.001-val_acc0.759.h5\"\n",
    "if weights_path:\n",
    "    model.load_weights(weights_path, by_name=True)#, skip_mismatch=True)\n",
    "    print('Load weights {}.'.format(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  44/1391 [..............................] - ETA: 10:19 - loss: 0.5167"
     ]
    }
   ],
   "source": [
    "model.fit(tfdataset_mapped_train, \n",
    "#                     validation_data=tfdataset_mapped_val,\n",
    "#                   steps_per_epoch=num_train // batch_size,\n",
    "                    epochs=total_epoch,\n",
    "                    initial_epoch=init_epoch,\n",
    "#                     workers=6,\n",
    "#                     use_multiprocessing=True,\n",
    "#                     max_queue_size=10,                    \n",
    "                    callbacks=callbacks\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b194006",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback.model = model\n",
    "eval_callback.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676818eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import hourglass_predict_keras, post_process_heatmap_simple\n",
    "heatmap = hourglass_predict_keras(model, data_item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.shape[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd106a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process_heatmap_simple(heatmap, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = heatmap[:, :,:, 0]\n",
    "np.where(_map == _map.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5822d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset_mapped_val.element_spec[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791fc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
